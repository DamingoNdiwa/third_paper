\documentclass[12pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{apacite}
\usepackage{natbib} 
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{caption}
\usepackage{verbatim}
%\onehalfspacing
\usepackage{float}
\usepackage{dsfont}
\DeclareUnicodeCharacter{03A7}{}



\title{Interpretation of the Wasserstein Impact Measure}
\author{Damian Mingo, Jack S. Hale}
\date{June 2023}


\usepackage{glossaries}
\DeclareUnicodeCharacter{03A7}{}

\newacronym{wim}{WIM}{Wasserstein Impact Measure}  
\newacronym{nuts}{NUTS}{No-U-Turn sampler}  
\newacronym{swim}{sWIM}{prior scaled Wasserstein Impact Measure}  
\newacronym{bf}{BF}{Bayes factor}
\newacronym{mcmc}{MCMC}{Markov Chain Monte Carlo}


\begin{document}


\maketitle


\begin{abstract}

To ensure parameter estimates are robust to prior specifications, it is essential to check if different priors produce varying results. We can investigate the effect of the prior on the posterior by utilizing a metric like the Wasserstein distance. Also, we need to determine if changes in parameter estimates are significant and if the priors have an impact.

One way to assess the effect of the prior is by examining the power posteriors. These posteriors result from raising the likelihood function to values between 0 and 1 inclusively. When the likelihood is raised to zero, it represents the prior distribution. When it is raised to one, it is the standard posterior. By raising the likelihood to smaller values, we can control the influence of the data and observe the impact of the prior on the inference. We can then estimate the summary statistics and the Wasserstein distance for each power posterior. In this way, we can determine the Wasserstein distance at which the parameter estimates differ significantly from the known parameters. These techniques will be first applied to conjugate cases and then extended to multiple linear regression.

Another approach is to scale the Wasserstein distance between two posteriors by the distance of their priors, where one of the priors is a baseline prior. This leads to the prior standardised Wasserstein Impact Measure (WIM). We can also know the prior WIM at which the parameter estimate changes from the known parameter values. 
\end{abstract}

\section{Introduction}

Several measures of prior impact assessment have been developed. These measures of prior impact are usually based on divergences such as the Kullbackâ€“Leibler divergence or distances such as the Wasserstein distance. However, these distances are not directly interpretable and cannot tell us the weight of the impact of a prior distribution. We provide the weight of impact for prior distributions relative to standard normal distributions.

\section{Methodology}
The central concept in this paper is power posteriors. For Bayesian inference, the sampling distribution can be raised to values between zero and one inclusive. The resulting posteriors are known as power posteriors. For power posteriors, the Bayes' theorem can be restated as shown in \cref{eq:pow_pos}.

\begin{equation}
\label{eq:pow_pos}
p(\theta|y) =  \frac{p(y|\theta)^\gamma p(\theta)}{z}, \quad \gamma \in [0, 1],
\end{equation}

where $p(\theta|y)$ is the power posterior, $p(y|\theta)$ is the likelihood function, $p(\theta)$ is the prior distribution, $z$ is the marginal likelihood, and $\gamma$ is a value that controls the influence of the data. Power posteriors have been used in many applications. One of such is the calculation of the marginal likelihood \citep{friel2008marginal}. They are used to calculate the objective \gls{bf} and are referred to there as fractional posteriors \citep{ohaganPropertiesIntrinsicFractional1997}. They have been applied to sample across multimodal distributions. We use the power posteriors to understand how prior impacts posterior inference. For conjugate cases, we derive and make use of analytic solutions. However, we use \gls{mcmc} to extend our approach for models with no closed-form solution. In addition, we compute the \gls{wim} of \citep{ghaderinezhadWassersteinImpactMeasure2022} and the \gls{swim} introduced in for interpretation of the impacts of priors on posterior inference.  The \gls{wim} and \gls{swim} require the calculation of Wasserstein distances between power posteriors. In addition to using, \gls{wim} and \gls{swim}, we use graphs to visualise the impacts of priors. One of such plots is the \gls{swim} vs $\gamma$. This plot is interesting because of its elbow-shaped curve similar to the scree plot used in principal component analysis. Hence, the values of \gls{swim} just before the elbow are of interest.



\subsection{Calculation of the Wasserstein distance}

\subsection{Normalisation with prior distances}

\subsection{Power posteriors}

\section{Examples}

\subsection{Normal-normal conjugate case with unknown mean}
Following standard arguments, it is possible to show that the distribution of
the power posterior for the following Bayesian model with dataset $x = (x_1,
\ldots, x_n)$
\begin{subequations}
\begin{align}
	x_1, \ldots, x_n &\overset{\mathrm{iid}}{\sim} \mathcal{N}(m, \sigma^2), \\
m &\sim \mathcal{N}(m_0, \sigma_0^2),
\end{align}
\end{subequations}
is normally distributed
\begin{equation*}
m \;|\; x \sim \mathcal{N} \left( \left( \frac{1}{\sigma_0^2} + \frac{\gamma n}{\sigma^2} \right)^{-1} \left(\frac{m_0}{\sigma_0^2} + \frac{\gamma n \bar{x}}{\sigma^2}  \right), \left( \frac{1}{\sigma_0^2} + \frac{\gamma n}{\sigma^2} \right)^{-1} \right),
\end{equation*}
where $\bar{x} = (\sum x_i)/ n$ is the sample mean. We remark that, as
expected, the prior is recovered when $\gamma = 0$ and the classic
normal-normal with unknown mean conjugate result is recovered when $\gamma =
1$. The role of $\gamma$ in this context is to reduce the contribution of each
element of the dataset through the likelihood. More specifically for the
normal-normal case, the effective data set size is reduced from the standard
posterior ($\gamma = 1$) from $n$ to $n \gamma$ in the power posterior. Note
however, regardless of the value of $\gamma > 0$, the entire dataset $x$ is
still used in the update from prior to the power posterior.

The 2-Wasserstein metric ($p = 2$) for two non-degenerate normal
measures $\mu_1$ and $\mu_2$ on $\mathbb{R}$ with means $m_1, m_2 \in
\mathbb{R}$ and variances $\sigma_1^2, \sigma_2^2 \in \mathbb{R}_{>0} :=
\left\lbrace x \in \mathbb{R} \; | \; x > 0 \right\rbrace$, respectively, can
be found in closed form as
\begin{equation*}
W_{2} (\mu_1, \mu_2)^2 = \left( m_1 - m_2 \right)^2 + \sigma_1^2 + \sigma_2^2 - 2 \bigl( \sigma_2 \sigma_1^2 \sigma_2 \bigr)^{1/2}.
\end{equation*}
Consequently in this case there is no need to resort to approximate numerical
computations to compute the Wasserstein metric. In closed form the Wasserstein
metric between the prior measure $\mu_0$ and the measure induced by the power posterior
$\mu_\gamma$ is
\begin{equation*}
W_2(\mu_0, \mu_\gamma)^2 = \sigma_{0}^{2} + \frac{\sigma^{2} \sigma_{0}^{2}}{p} + \frac{\left(\gamma m_{0} n \sigma_{0}^{2} - \gamma n \sigma_{0}^{2} \bar{x}\right)^{2}}{p^{2}} - \frac{2 \sigma \sigma_{0}^{2}}{\sqrt{p}}
\end{equation*}
with $p = \gamma n \sigma_{0}^{2} + \sigma^{2}$ and the derivative of the
squared Wasserstein distance with respect to $\gamma$ given by
\tiny
\begin{equation*}
\frac{d}{d\gamma} [W_2(\mu_0, \mu_\gamma)^2] =
p^{-\frac{15}{2}}n \sigma_{0}^{2} \left(- p^{\frac{11}{2}} \sigma^{2} \sigma_{0}^{2} - 2 p^{\frac{9}{2}} \left(- \gamma n \sigma_{0}^{2} \bar{x} + m_{0} p - m_{0} \sigma^{2}\right) \left(- \gamma n \sigma_{0}^{2} \bar{x} - m_{0} \sigma^{2} + p \bar{x}\right) + p^{6} \sigma \sigma_{0}^{2}\right).
\end{equation*}
\normalsize
Letting $\gamma = 0$ it can be verified that $W_2(\mu_0, \mu_0)^2 = 0$, as
expected, and also that $\frac{d}{d\gamma}[W_2(\mu_0, \mu_0)^2] = 0$. 

In the first experiment we generate a (small) dataset of size $n = 10$ from a
normal distribution with zero mean and unit variance. We propose three prior
choices by adjusting the prior parameters $m_0$ and $\sigma_0^2$:
\begin{enumerate}
\item \emph{Non-informative prior}. $m_0 = 0$ and $\sigma_0^2 = 100$. With
this relatively flat prior we expect the posterior to be `data prominent'
and highly sensitive to the inclusion of information via the likelihood,
here controlled by the parameter $\gamma$.
\item \emph{Informative `correct' prior}. $m_0 = 0$ and $\sigma_0^2 = 2$.
This prior expresses definite information about the parameter that
coincidentally coincides with the true parameter. 
\item \emph{Informative `incorrect' prior}. $m_0 = -5$ and $\sigma_0^2 = 2$.
This prior expresses definite information about the parameter that
does not coincide with the true parameter value. 
\end{enumerate}
In \cref{fig:normal_normal_wasserstein_distance_different_priors} we calculate
the squared 2-Wasserstein distance between the posterior measure $\mu_\gamma$
for varying values of $\gamma$ and under the three prior assumptions just
described. All three distances appear to be monotonically increasing in
$\gamma$. The distance between the posterior and prior as $\gamma \to 1$ is
largest ($\sim10^2$) for the non-informative prior, and smallest ($\sim 1$) for
the informative `correct' prior. For the non-informative prior as $\gamma$ is
increased from $0$ we can see a very sudden increase in the metric, which
supports the intuition that the posterior is `data prominent' or `data
sensitive`. By comparison, the distance for the informative `correct' prior
evolves to its final distance more slowly, reflecting that the information
contained in the prior does not strongly disagree with that in the data. The
informative `incorrect' prior sits in between the two extreme cases.
\begin{figure}
\begin{center}
\includegraphics{imgs/normal_normal_wasserstein_distance_different_priors.pdf}
\end{center}
\caption{Normal-normal conjugate case; plot showing squared
2-Wasserstein metric between power posterior and prior against the
power posterior parameter $\gamma \in [0, \frac{1}{2}]$ under the three
different prior assumptions described in the text.}\label{fig:normal_normal_wasserstein_distance_different_priors}
\end{figure}

Given the intepretation of the parameter $\gamma n$ as an effective sample size
in the normal-normal setting, we briefly contrast the power posterior approach
with an approach based on using the standard posterior and varying the size $n$
of the dataset.

\subsection{Inverse-Gamma conjugate case with unknown variance}
Following standard arguments, it is possible to show that the distribution of
the power posterior for the following Bayesian model with dataset $x = (x_1,
\ldots, x_n)$
\begin{subequations}
\begin{align}
x_1, \ldots, x_n &\overset{\mathrm{iid}}{\sim} \mathcal{N}(m, \sigma^2), \\
\sigma^2 &\sim \text{Inv-Gamma}(\alpha, \beta)
\end{align}
\end{subequations}
with $\alpha, \beta \in \mathbb{R}_{>0}$, is distributed according to an
inverse gamma distribution
\begin{equation*}
\sigma^2 \; | \; x_1, \ldots, x_n \sim \text{Inv-Gamma}\left( \alpha + \frac{\gamma n}{2}, \beta + \frac{\gamma n}{2} s^2 \right). 
\end{equation*}
where $s^2 = (\sum_{i} \left( x_i - \mu \right)^2)/n$ is the uncorrected sample
variance. Again, setting $\gamma = 0$ recovers the prior, and setting $\gamma =
1$ recovers the usual conjugate result for the standard Bayesian posterior.

To the best of our knowledge there is no easily computable expression for the
Wasserstein metric between inverse gamma distributions for any value of $p$.
We instead use the Vallender formula which relates the 1-Wasserstein distance
between two probability measures $\mu_1$ and $\mu_2$ on $\mathbb{R}$ with
cumulative distribution functions $F_1(x)$ and $F_2(x)$, respectively
\begin{equation}
W_1(\mu_1, \mu_2) = \int_{\mathbb{R}} | F_1(x) - F_2(x) | \; \mathrm{d}x.
\end{equation}


\subsection{Poisson-Gamma conjugate case with unknown rate}
The distribution of the power posterior of the following Bayesian model with
dataset $x = (x_1, \ldots, x_n)$ 
\begin{subequations}
\begin{align}
x_1, \ldots, x_n &\overset{\mathrm{iid}}{\sim} \text{Poisson}(\lambda), \\
\lambda &\sim \text{Gamma}(\alpha, \beta),
\end{align}
\end{subequations}
where $\lambda \in \mathbb{R}_{>0}$ is the unknown rate parameter, and $\alpha,
\beta \in \mathbb{R}_{>0}$ are prior parameters is given by
\begin{equation}
\lambda \; | \; x \sim \text{Gamma} \left( \alpha + \gamma n \bar{x}, \beta + \gamma n \right)
\end{equation}

\begin{figure}[htbp]
  \centering
  \includegraphics{imgs/wasserstein_distance.pdf} 
  \caption{Poisson-Gamma conjugate case with unknown rate; plot showing squared 2-Wasserstein metric between power posterior and prior against the power posterior parameter $\gamma \in [0, \frac{1}{2}]$ under the three different prior assumptions described in the text.} 
  \label{fig:poisson_ga} 
\end{figure}


\begin{figure}[htbp]
  \centering
  \includegraphics{imgs/swim_poisson_gamma_con.pdf} 
  \caption{Poisson-Gamma conjugate case with unknown rate; Plot showing the prior scale WIM between power posteriors with a non-informative prior as baseline against the power posterior parameter $\gamma \in [0, \frac{1}{2}]$. The sWIM value of 0.40 seems to be the elbow point where the plots levels off, similar to the scree plot in Principal component analysis.} 
  \label{fig:swim_poisson_ga} 
\end{figure}
\clearpage

\subsection{Skew-normal distribution}
To gain more insight into prior impacts, we analysed the frontiers data set found in the R package sn \citep{azzalini2023}. This data set has been analysed by \cite{ghaderinezhadWassersteinImpactMeasure2022}. The data set is interesting because the maximum likelihood estimate is unrealistic. The data is generated by drawing 50 samples from a skew-normal distribution with a skewness parameter of 5. The density of a random variable $x$ that follows a skew normal distribution \citep{azzaliniClassD1985} is in \cref{skew_n}. Where $\phi$ is the standard normal probability density function, $\Phi$ is the cumulative distribution function of the standard normal distribution, $\mu$ is the location, $\sigma$ is the scale parameter and $\alpha$ is the skewness parameter.
\begin{equation}
    f(x; \alpha) = \frac{2}{\sigma} \phi \left( \frac{x - \mu}{\sigma} \right) \Phi \left( \alpha \frac{(x - \mu)}{\sigma}  \right), \quad x \in \mathbb{R}
    \label{skew_n}
\end{equation}
We fit the skew-normal distribution to the data with different sets of priors for the skewness parameter as in \cite{ghaderinezhadWassersteinImpactMeasure2022}. We compare the Wasserstein distance between various power posteriors and the prior. The priors are: 
\begin{itemize}
\item Uniform prior
\item Jeffreys prior
\item Bayes-Laplace prior
\item Beta total variation prior (BTV)
\item Normal prior
\end{itemize}
The squared Wasserstein distance between the priors and the power posteriors are in \cref{fig:skew_diff_priors}. The results suggest the difference between the Uniform and the Jeffrys priors is very small. However, the distance between the prior ($\gamma=0$) and the power posteriors increases with an increase in $\gamma$ but becomes stable as $\gamma$ approaches 1. These insights are more noticeable in the \gls{swim} compared to the squared Wasserstein distances.
\begin{figure}[h]
\begin{center}
\includegraphics{imgs/wasser_distskew.pdf}
\end{center}
\caption{Squared Wasserstein distance between the prior and various power posteriors for different values $\gamma \in [0, 0.5]$ on the frontiers data set.}\label{fig:skew_diff_priors}
\end{figure}
Hence, we also explore the concept of \gls{swim} introduced in ODE paper. We use Jeffrys prior and the Uniform prior as baseline priors for this. The results are in plots \cref{fig:swimdistskew}. We can observe that the \gls{swim} drops to zero as the prior transitions to the posterior. That is, as $\gamma$ increases from 0 to 0.5. When the Jeffreys prior is used as the baseline, the \gls{swim}  drops from 2.0 (\cref{fig:swimdistskew} A) to near zero. When the baseline is a uniform uniform prior, the \gls{swim} drops from 1.0 to closer to 0.2. Hence, one can infer that the \gls{swim} is closer to zero when there is no impact compared to a baseline prior. Also, the \gls{swim} is closer to or greater than 1.0 when the prior impact is different from the baseline prior. When the baseline prior is noninformative, a \gls{swim} value close to zero indicates the prior in question is noninformative. This is seen in (\cref{fig:swimdistskew} A), where the dashed line seems to have a constant slope very close to zero for all values of $\gamma$. The inset shows the \gls{swim} values are very close to zero. When the baseline is a uniform distribution, the \gls{swim} are below one but have a minimum value of 0.2, which is further from zero compared to when the baseline is Jefferys prior. The fact that the \gls{swim} for the Uniform and the Jeffreys prior is close to zero, as shown by the dashed line in \cref{fig:swimdistskew}A, confirms that these two priors are closest. Another interesting visual is the density plot of the priors and the power posteriors. Plots of the two closest priors, the Uniform and Jeffreys priors, illustrate how the prior density transitions to the posterior. For the Uniform prior, the mode is not visible, but as $\gamma$ starts increasing, a mode becomes visible, and the Uniform distribution starts transitioning into a skew distribution. On the other hand, the mode is visible for Jeffreys prior, but there is no skewness. As $\gamma$ increases, the mode becomes more visible, and skewness becomes visible. As the value of $\gamma$ increases, the mode becomes more peaked, and the range of the skewness parameter decreases.





\begin{figure}[h]
\begin{center}
\includegraphics{imgs/swim_distskew.pdf}
\end{center}
\caption{Prior scale WIM for the skewness parameter of the skew normal distribution with \textbf{(A)} Jeffrys prior as the baseline prior and \textbf{(B)} the uniform prior as the baseline prior. For (A), 0.40 is the sWIM value at which the elbow levels off similar to principal component analysis.}\label{fig:swimdistskew}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics{imgs/uniform_pow.pdf}
\end{center}
\caption{Marginal posteriors for the skewness parameter show the transition from Uniform prior ($\gamma=0$) to various power posteriors ($0<\gamma\leq1$).  Posterior based on the frontier data set for the skew-normal distribution.}\label{fig:skew_normal_powpos}
\end{figure}


\begin{figure}[h]
\begin{center}
\includegraphics{imgs/Jeffreys.pdf}
\end{center}
\caption{Marginal posteriors for the skewness parameter show the transition from Jeffreys prior ($\gamma=0$) to various power posteriors ($0<\gamma\leq1$).  Posterior based on the frontier data set for the skew-normal distribution. }\label{fig:skew_jeff_powpos}
\end{figure}

\clearpage
\subsection{Simple linear regression}

\begin{subequations}
\begin{align}
        Y|\beta_0, \beta_1,  \epsilon  &\sim \mathcal{N} (\beta_0 + \beta_1 X_1,  \epsilon)\\
    \beta_0  &\sim \mathcal{N}(0.0, 10.0)\\
     \beta_1  &\sim \mathcal{N}(0.0, 10.0)\\
     \epsilon & \sim \text{IG}(1.0, 0.5)
\end{align}
\end{subequations}

\begin{figure}[h]
\begin{center}
\includegraphics{imgs/swimgamma.pdf}
\caption{There is a sharp change in the sWIM closer to 0.40. This is similar to the previous examples when the elbow level off close to 0.40. }
\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}
\includegraphics{imgs/betagamma.pdf}
\end{center}
\end{figure}

\section{Conclusion}
\clearpage
\bibliographystyle{apacite}
\bibliography{ref}

\end{document}
