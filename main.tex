\documentclass[12pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{apacite}
\usepackage{natbib} 
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{caption}
\usepackage{verbatim}
%\onehalfspacing
\usepackage{float}
\usepackage{dsfont}
\DeclareUnicodeCharacter{03A7}{}



\title{Intepretation of the Wasserstein Impact Measure}
\author{Damian Mingo, Jack S. Hale}
\date{June 2023}


\usepackage{glossaries}
\DeclareUnicodeCharacter{03A7}{}

\newacronym{wim}{WIM}{Wasserstein Impact Measure}  
\newacronym{nuts}{NUTS}{No-U-Turn sampler}  
\newacronym{swim}{sWIM}{prior scaled Wasserstein Impact Measure}  
\newacronym{bf}{BF}{Bayes factor}
\newacronym{mcmc}{MCMC}{Markov Chain Monte Carlo}


\begin{document}


\maketitle


\begin{abstract}

To ensure parameter estimates are robust to prior specifications, it is essential to check if different priors produce varying results. We can investigate the effect of the prior on the posterior by utilizing a metric like the Wasserstein distance. Also, we need to determine if changes in parameter estimates are significant and if the priors have an impact.

One way to assess the effect of the prior is by examining the power posteriors. These posteriors result from raising the likelihood function to values between 0 and 1 inclusively. When the likelihood is raised to zero, it represents the prior distribution. When it is raised to one, it is the standard posterior. By raising the likelihood to smaller values, we can control the influence of the data and observe the impact of the prior on the inference. We can then estimate the summary statistics and the Wasserstein distance for each power posterior. In this way, we can determine the Wasserstein distance at which the parameter estimates differ significantly from the known parameters. These techniques will be first applied to conjugate cases and then extended to multiple linear regression.

Another approach is to scale the Wasserstein distance between two posteriors by the distance of their priors, where one of the priors is a baseline prior. This leads to the prior standardised Wasserstein Impact Measure (WIM). We can also know the prior WIM at which the parameter estimate changes from the known parameter values. 
\end{abstract}

\section{Introduction}

Several measures of prior impact assessment have been developed. These measures of prior impact are usually based on divergences such as the Kullbackâ€“Leibler divergence or distances such as the Wasserstein distance. However, these distances are not directly interpretable and cannot tell us the weight of the impact of a prior distribution. We provide the weight of impact for prior distributions relative to standard normal distributions.

\section{Methodology}
The central concept in this paper is power posteriors. For Bayesian inference, the sampling distribution can be raised to values between zero and one inclusive. The resulting posteriors are known as power posteriors. Power posteriors have been used in many applications. One of such is the calculation of the marginal likelihood \citep{friel2008marginal}. They are used to calculate the objective \gls{bf} and are referred to there as fractional posteriors \citep{ohaganPropertiesIntrinsicFractional1997}. They have been applied to sample across multimodal distributions. We use the power posteriors to understand how prior impacts posterior inference. For conjugate cases, we derive and make use of analytic solutions. However, we use \gls{mcmc} to extend our approach for models with no closed-form solution. In addition, we compute the \gls{wim} of \citep{ghaderinezhadWassersteinImpactMeasure2022} and the \gls{swim} introduced in for interpretation of the impacts of priors on posterior inference.   The \gls{wim} and \gls{swim} require the calculation of Wasserstein distances between power posteriors.

\begin{equation}
p(\theta|y) =  \frac{p(y|\theta)^\gamma p(\theta)}{z}, \quad \gamma \in [0, 1]
\end{equation}

where $p(\theta|y)$ is the power posterior, $p(y|\theta)$ is the likelihood function, $p(\theta)$ is the prior distribution, $z$ is the marginal likelihood, and $\gamma$ is a value that controls the influence of the data.


\subsection{Calculation of the Wasserstein distance}

\subsection{Normalisation with prior distances}

\subsection{Power posteriors}

\section{Examples}

\subsection{Normal-normal conjugate case with unknown mean}
Following standard arguments, it is possible to show that the distribution of
the power posterior for the following Bayesian model with dataset $x = (x_1,
\ldots, x_n)$
\begin{subequations}
\begin{align}
	x_1, \ldots, x_n &\overset{\mathrm{iid}}{\sim} \mathcal{N}(m, \sigma^2), \\
m &\sim \mathcal{N}(m_0, \sigma_0^2),
\end{align}
\end{subequations}
is normally distributed
\begin{equation*}
m \;|\; x \sim \mathcal{N} \left( \left( \frac{1}{\sigma_0^2} + \frac{\gamma n}{\sigma^2} \right)^{-1} \left(\frac{m_0}{\sigma_0^2} + \frac{\gamma n \bar{x}}{\sigma^2}  \right), \left( \frac{1}{\sigma_0^2} + \frac{\gamma n}{\sigma^2} \right)^{-1} \right),
\end{equation*}
where $\bar{x} = (\sum x_i)/ n$ is the sample mean. We remark that, as
expected, the prior is recovered when $\gamma = 0$ and the classic
normal-normal with unknown mean conjugate result is recovered when $\gamma =
1$. The role of $\gamma$ in this context is to reduce the contribution of each
element of the dataset through the likelihood. More specifically for the
normal-normal case, the effective data set size is reduced from the standard
posterior ($\gamma = 1$) from $n$ to $n \gamma$ in the power posterior. Note
however, regardless of the value of $\gamma > 0$, the entire dataset $x$ is
still used in the update from prior to the power posterior.

The 2-Wasserstein metric ($p = 2$) for two non-degenerate normal
measures $\mu_1$ and $\mu_2$ on $\mathbb{R}$ with means $m_1, m_2 \in
\mathbb{R}$ and variances $\sigma_1^2, \sigma_2^2 \in \mathbb{R}_{>0} :=
\left\lbrace x \in \mathbb{R} \; | \; x > 0 \right\rbrace$, respectively, can
be found in closed form as
\begin{equation*}
W_{2} (\mu_1, \mu_2)^2 = \left( m_1 - m_2 \right)^2 + \sigma_1^2 + \sigma_2^2 - 2 \bigl( \sigma_2 \sigma_1^2 \sigma_2 \bigr)^{1/2}.
\end{equation*}
Consequently in this case there is no need to resort to approximate numerical
computations to compute the Wasserstein metric. In closed form the Wasserstein
metric between the prior measure $\mu_0$ and the measure induced by the power posterior
$\mu_\gamma$ is
\begin{equation*}
W_2(\mu_0, \mu_\gamma)^2 = \sigma_{0}^{2} + \frac{\sigma^{2} \sigma_{0}^{2}}{p} + \frac{\left(\gamma m_{0} n \sigma_{0}^{2} - \gamma n \sigma_{0}^{2} \bar{x}\right)^{2}}{p^{2}} - \frac{2 \sigma \sigma_{0}^{2}}{\sqrt{p}}
\end{equation*}
with $p = \gamma n \sigma_{0}^{2} + \sigma^{2}$ and the derivative of the
squared Wasserstein distance with respect to $\gamma$ given by
\tiny
\begin{equation*}
\frac{d}{d\gamma} [W_2(\mu_0, \mu_\gamma)^2] =
p^{-\frac{15}{2}}n \sigma_{0}^{2} \left(- p^{\frac{11}{2}} \sigma^{2} \sigma_{0}^{2} - 2 p^{\frac{9}{2}} \left(- \gamma n \sigma_{0}^{2} \bar{x} + m_{0} p - m_{0} \sigma^{2}\right) \left(- \gamma n \sigma_{0}^{2} \bar{x} - m_{0} \sigma^{2} + p \bar{x}\right) + p^{6} \sigma \sigma_{0}^{2}\right).
\end{equation*}
\normalsize
Letting $\gamma = 0$ it can be verified that $W_2(\mu_0, \mu_0)^2 = 0$, as
expected, and also that $\frac{d}{d\gamma}[W_2(\mu_0, \mu_0)^2] = 0$. 

In the first experiment we generate a (small) dataset of size $n = 10$ from a
normal distribution with zero mean and unit variance. We propose three prior
choices by adjusting the prior parameters $m_0$ and $\sigma_0^2$:
\begin{enumerate}
\item \emph{Non-informative prior}. $m_0 = 0$ and $\sigma_0^2 = 100$. With
this relatively flat prior we expect the posterior to be `data prominent'
and highly sensitive to the inclusion of information via the likelihood,
here controlled by the parameter $\gamma$.
\item \emph{Informative `correct' prior}. $m_0 = 0$ and $\sigma_0^2 = 2$.
This prior expresses definite information about the parameter that
coincidentally coincides with the true parameter. 
\item \emph{Informative `incorrect' prior}. $m_0 = -5$ and $\sigma_0^2 = 2$.
This prior expresses definite information about the parameter that
does not coincide with the true parameter value. 
\end{enumerate}
In \cref{fig:normal_normal_wasserstein_distance_different_priors} we calculate
the squared 2-Wasserstein distance between the posterior measure $\mu_\gamma$
for varying values of $\gamma$ and under the three prior assumptions just
described. All three distances appear to be monotonically increasing in
$\gamma$. The distance between the posterior and prior as $\gamma \to 1$ is
largest ($\sim10^2$) for the non-informative prior, and smallest ($\sim 1$) for
the informative `correct' prior. For the non-informative prior as $\gamma$ is
increased from $0$ we can see a very sudden increase in the metric, which
supports the intuition that the posterior is `data prominent' or `data
sensitive`. By comparison, the distance for the informative `correct' prior
evolves to its final distance more slowly, reflecting that the information
contained in the prior does not strongly disagree with that in the data. The
informative `incorrect' prior sits in between the two extreme cases.
\begin{figure}
\begin{center}
\includegraphics{imgs/normal_normal_wasserstein_distance_different_priors.pdf}
\end{center}
\caption{Normal-normal conjugate case; plot showing squared
2-Wasserstein metric between power posterior and prior against the
power posterior parameter $\gamma \in [0, \frac{1}{2}]$ under the three
different prior assumptions described in the text.}\label{fig:normal_normal_wasserstein_distance_different_priors}
\end{figure}

Given the intepretation of the parameter $\gamma n$ as an effective sample size
in the normal-normal setting, we briefly contrast the power posterior approach
with an approach based on using the standard posterior and varying the size $n$
of the dataset.

\subsection{Inverse-Gamma conjugate case with unknown variance}
Following standard arguments, it is possible to show that the distribution of
the power posterior for the following Bayesian model with dataset $x = (x_1,
\ldots, x_n)$
\begin{subequations}
\begin{align}
x_1, \ldots, x_n &\overset{\mathrm{iid}}{\sim} \mathcal{N}(m, \sigma^2), \\
\sigma^2 &\sim \text{Inv-Gamma}(\alpha, \beta)
\end{align}
\end{subequations}
with $\alpha, \beta \in \mathbb{R}_{>0}$, is distributed according to an
inverse gamma distribution
\begin{equation*}
\sigma^2 \; | \; x_1, \ldots, x_n \sim \text{Inv-Gamma}\left( \alpha + \frac{\gamma n}{2}, \beta + \frac{\gamma n}{2} s^2 \right). 
\end{equation*}
where $s^2 = (\sum_{i} \left( x_i - \mu \right)^2)/n$ is the uncorrected sample
variance. Again, setting $\gamma = 0$ recovers the prior, and setting $\gamma =
1$ recovers the usual conjugate result for the standard Bayesian posterior.

To the best of our knowledge there is no easily computable expression for the
Wasserstein metric between inverse gamma distributions for any value of $p$.
We instead use the Vallender formula which relates the 1-Wasserstein distance
between two probability measures $\mu_1$ and $\mu_2$ on $\mathbb{R}$ with
cumulative distribution functions $F_1(x)$ and $F_2(x)$, respectively
\begin{equation}
W_1(\mu_1, \mu_2) = \int_{\mathbb{R}} | F_1(x) - F_2(x) | \; \mathrm{d}x.
\end{equation}


\subsection{Poisson-Gamma conjugate case with unknown rate}
The distribution of the power posterior of the following Bayesian model with
dataset $x = (x_1, \ldots, x_n)$ 
\begin{subequations}
\begin{align}
x_1, \ldots, x_n &\overset{\mathrm{iid}}{\sim} \text{Poisson}(\lambda), \\
\lambda &\sim \text{Gamma}(\alpha, \beta),
\end{align}
\end{subequations}
where $\lambda \in \mathbb{R}_{>0}$ is the unknown rate parameter, and $\alpha,
\beta \in \mathbb{R}_{>0}$ are prior parameters is given by
\begin{equation}
\lambda \; | \; x \sim \text{Gamma} \left( \alpha + \gamma n \bar{x}, \beta + \gamma n \right)
\end{equation}

\subsection{Simple linear regression}
    
\begin{subequations}
\begin{align}
        Y|\beta_0, \beta_1,  \epsilon  &\sim \mathcal{N} (\beta_0 + \beta_1 X_1,  \epsilon)\\
    \beta_0  &\sim \mathcal{N}(0.0, 10.0)\\
     \beta_1  &\sim \mathcal{N}(0.0, 10.0)\\
     \epsilon & \sim \text{IG}(1.0, 0.5)
\end{align}
\end{subequations}

\begin{figure}
\begin{center}
\includegraphics{imgs/swimgamma.pdf}
\end{center}
\end{figure}
\section{Conclusion}

\bibliographystyle{apacite}
\bibliography{ref}

\end{document}
