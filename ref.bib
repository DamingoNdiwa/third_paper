@inproceedings{NIPS2013_af21d0c9,
  title = {Sinkhorn Distances: {{Lightspeed}} Computation of Optimal Transport},
  booktitle = {{Advances in Neural Information Processing Systems}},
  author = {Cuturi, Marco},
  editor = {Burges, C.J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K.Q.},
  year = {2013},
  volume = {26},
  url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf},
  publisher = {{Curran Associates, Inc.}}}


@article{cuturi2022optimal,
  title={{Optimal Transport Tools} ({OTT}): A {JAX} Toolbox for all things {Wasserstein}},
  author={Cuturi, Marco and Meng-Papaxanthos, Laetitia and Tian, Yingtao and Bunne, Charlotte and
          Davis, Geoff and Teboul, Olivier},
  journal={arXiv preprint arXiv:2201.12324},
  year={2022}
}

@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@article{vallender_calculation_1974,
	title = {Calculation of the {Wasserstein} {Distance} {Between} {Probability} {Distributions} on the {Line}},
	volume = {18},
	issn = {0040-585X},
	doi = {10.1137/1118101},
	number = {4},
	urldate = {2024-07-12},
	journal = {Theory of Probability \& Its Applications},
	author = {Vallender, S. S.},
	year = {1974},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {784--786},
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{ghorbanzadeh_method_2014,
	title = {A {Method} to {Simulate} the {Skew} {Normal} {Distribution}},
	volume = {05},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {2152-7385, 2152-7393},
	doi = {10.4236/am.2014.513201},
	number = {13},
	urldate = {2024-07-12},
	journal = {Applied Mathematics},
	author = {Ghorbanzadeh, Dariush and Jaupi, Luan and Durand, Philippe},
	year = {2014},
	pages = {2073--2076},
}


@software{deepmind2020jax,
  title = {The {D}eep{M}ind {JAX} {E}cosystem},
  author = {DeepMind and Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu, Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Sartran, Laurent and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stanojevi\'{c}, Milo\v{s} and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and Viola, Fabio},
  url = {http://github.com/deepmind},
  year = {2020},
}

@article{dillonTensorFlowDistributions2017,
  title = {{{TensorFlow Distributions}}},
  author = {Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Brevdo, Eugene and Vasudevan, Srinivas and Moore, Dave and Patton, Brian and Alemi, Alex and Hoffman, Matt and Saurous, Rif A.},
  year = {2017},
  journal = {{arXiv}},
  doi = {10.48550/ARXIV.1711.10604},
  urldate = {2023-10-01},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Machine Learning (cs.LG),Machine Learning (stat.ML),Programming Languages (cs.PL)}
}

@article{hoffman2014,
  title = {The {{No-U-Turn}} Sampler: Adaptively Setting Path Lengths in {{Hamiltonian Monte Carlo}}.},
  author = {Hoffman, Matthew D and Gelman, Andrew},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {15},
  number = {1},
  pages = {1593--1623}
}

@article{friel2008marginal,
  title = {Marginal {{Likelihood Estimation}} via {{Power Posteriors}}},
  author = {Friel, N. and Pettitt, A. N.},
  year = {2008},
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {70},
  number = {3},
  pages = {589--607},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2007.00650.x},
  urldate = {2023-08-07},
  abstract = {Summary             Model choice plays an increasingly important role in statistics. From a Bayesian perspective a crucial goal is to compute the marginal likelihood of the data for a given model. However, this is typically a difficult task since it amounts to integrating over all model parameters. The aim of the paper is to illustrate how this may be achieved by using ideas from thermodynamic integration or path sampling. We show how the marginal likelihood can be computed via Markov chain Monte Carlo methods on modified posterior distributions for each model. This then allows Bayes factors or posterior model probabilities to be calculated. We show that this approach requires very little tuning and is straightforward to implement. The new method is illustrated in a variety of challenging statistical settings.},
  langid = {english}
}

@article{ohaganPropertiesIntrinsicFractional1997,
  title = {Properties of Intrinsic and Fractional {{Bayes}} Factors},
  author = {O’Hagan, Anthony.},
  year = {1997},
  journal = {Test},
  shortjournal = {Test},
  volume = {6},
  number = {1},
  pages = {101--118},
  issn = {1133-0686, 1863-8260},
  doi = {10.1007/BF02564428},
  urldate = {2023-08-01},
    publisher={Springer},
  langid = {english}
}


@article{ghaderinezhadWassersteinImpactMeasure2022,
  title = {The {{Wasserstein Impact Measure}} ({{WIM}}): {{A}} Practical Tool for Quantifying Prior Impact in {{Bayesian}} Statistics},
  author = {Ghaderinezhad, Fatemeh and Ley, Christophe and Serrien, Ben},
  year = {2022},
  journal = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  volume = {174},
  pages = {107352},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2021.107352},
  abstract = {The prior distribution is a crucial building block in Bayesian analysis, and its choice will impact the subsequent inference. It is therefore important to have a convenient way to quantify this impact, as such a measure of prior impact will help to choose between two or more priors in a given situation. To this end a new approach, the Wasserstein Impact Measure (WIM), is introduced. In three simulated scenarios, the WIM is compared to two competitor prior impact measures from the literature, and its versatility is illustrated via two real datasets.},
  keywords = {Effective sample size,Neutrality,Prior distribution,Vallender formula,Wasserstein distance}
}

@article{azzalini2023,
  title={R package ‘sn’},
  author={Azzalini, Adelchi},
  journal={The Skew-Normal and Related Distributions Such as the Skew-t and
the SUN},
  year={2023},
  url={http://azzalini.stat.unipd.it/SN/}
}


@article{azzaliniClassD1985,
  title = {A {{Class}} of {{Distributions Which Includes}} the {{Normal Ones}}},
  author = {Azzalini, A.},
  year = {1985},
  journal = {Scandinavian Journal of Statistics},
  volume = {12},
  number = {2},
  eprint = {4615982},
  eprinttype = {jstor},
  pages = {171--178},
  publisher = {{[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]}},
  issn = {03036898, 14679469},
  url = {http://www.jstor.org/stable/4615982},
  abstract = {[A new class of density functions depending on a shape parameter λ is introduced, such that λ=0 corresponds to the standard normal density. The properties of this class of density functions are studied.]}
}

@article{Joseph_2000,
 ISSN = {08834237},
 URL = {http://www.jstor.org/stable/2676676},
 abstract = {We propose a general class of prior distributions for arbitrary regression models. We discuss parametric and semiparametric models. The prior specification for the regression coefficients focuses on observable quantities in that the elicitation is based on the availability of historical data D0 and a scalar quantity a0 quantifying the uncertainty in D0. Then D0 and a0 are used to specify a prior for the regression coefficients in a semiautomatic fashion. The most natural specification of D0 arises when the raw data from a similar previous study are available. The availability of historical data is quite common in clinical trials, carcinogenicity studies, and environmental studies, where large data bases are available from similar previous studies. Although the methodology we present here is quite general, we will focus only on using historical data from similar previous studies to construct the prior distributions. The prior distributions are based on the idea of raising the likelihood function of the historical data to the power a0, where ≤ a0 ≤ 1. We call such prior distributions power prior distributions. We examine the power prior for four commonly used classes of regression models. These include generalized linear models, generalized linear mixed models, semiparametric proportional hazards models, and cure rate models for survival data. For these classes of models, we discuss the construction of the power prior, prior elicitation issues, propriety conditions, model selection, and several other properties. For each class of models, we present real data sets to demonstrate the proposed methodology.},
 author = {Joseph G. Ibrahim and Ming-Hui Chen},
 journal = {Statistical Science},
 number = {1},
 pages = {46--60},
 publisher = {Institute of Mathematical Statistics},
 title = {Power Prior Distributions for Regression Models},
 urldate = {2024-07-22},
 volume = {15},
 year = {2000}
}


@article{Dimitris_2015,
author = {Dimitris Fouskakis and Ioannis Ntzoufras and David Draper},
title = {{Power-Expected-Posterior Priors for Variable Selection in Gaussian Linear Models}},
volume = {10},
journal = {Bayesian Analysis},
number = {1},
publisher = {International Society for Bayesian Analysis},
pages = {75 -- 107},
keywords = {Bayes factors, Bayesian variable selection, consistency, expected-posterior priors, Gaussian linear models, g-prior, Hyper-g prior, Lasso, Non-local priors, Power-prior, Prior compatibility, SCAD, training samples, unit-information prior},
year = {2015},
doi = {10.1214/14-BA887}}

@article{Jeffrey_2019,
author = {Jeffrey W. Miller and David B. Dunson},
title = {Robust Bayesian Inference via Coarsening},
journal = {Journal of the American Statistical Association},
volume = {114},
number = {527},
pages = {1113--1125},
year = {2019},
publisher = {Taylor \& Francis},
doi = {10.1080/01621459.2018.1469995},
note ={PMID: 31942084},
eprint = { https://doi.org/10.1080/01621459.2018.1469995
}}

@article{yaoReviewOptimalSubsampling2021,
	title = {A {{Review}} on {{Optimal Subsampling Methods}} for {{Massive Datasets}}},
	author = {Yao, Yaqiong and Wang, HaiYing},
	year = {2021},
	journal = {Journal of Data Science},
	pages = {151--172},
	issn = {1680-743X, 1683-8602},
	doi = {10.6339/21-JDS999},
	urldate = {2024-07-23},
	langid = {english},
	file = {/Users/damian.ndiwago/Zotero/storage/PVC23TSD/Yao and Wang - 2021 - A Review on Optimal Subsampling Methods for Massiv.pdf}
}

@inproceedings{Drineas_2006,
	author = {Drineas, Petros and Mahoney, Michael W. and Muthukrishnan, S.},
	title = {Sampling algorithms for l2 regression and applications},
	year = {2006},
	isbn = {0898716055},
	publisher = {Society for Industrial and Applied Mathematics},
	address = {USA},
	abstract = {We present and analyze a sampling algorithm for the basic linear-algebraic problem of l2 regression. The l2 regression (or least-squares fit) problem takes as input a matrix A ∈ Rn\texttimes{}d (where we assume n > d) and a target vector b ∈ Rn, and it returns as output Z = minx∈Rd |b - Ax|2. Also of interest is xopt = A+b, where A+ is the Moore-Penrose generalized inverse, which is the minimum-length vector achieving the minimum. Our algorithm randomly samples r rows from the matrix A and vector b to construct an induced l2 regression problem with many fewer rows, but with the same number of columns. A crucial feature of the algorithm is the nonuniform sampling probabilities. These probabilities depend in a sophisticated manner on the lengths, i.e., the Euclidean norms, of the rows of the left singular vectors of A and the manner in which b lies in the complement of the column space of A. Under appropriate assumptions, we show relative error approximations for both Z and xopt. Applications of this sampling methodology are briefly discussed.},
	booktitle = {Proceedings of the Seventeenth Annual {ACM-SIAM} Symposium on Discrete Algorithm},
	pages = {1127–1136},
	numpages = {10},
	location = {Miami, Florida},
	series = {SODA '06}
}


@article{wangOptimalSubsampling2018,
	title = {Optimal {{Subsampling}} for {{Large Sample Logistic Regression}}},
	author = {Wang, HaiYing and Zhu, Rong and Ma, Ping},
	year = {2018},
	journal = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	volume = {113},
	number = {522},
	pages = {829--844},
	issn = {0162-1459, 1537-274X},
	doi = {10.1080/01621459.2017.1292914},
	urldate = {2024-07-23},
	langid = {english},
	file = {/Users/damian.ndiwago/Zotero/storage/7UY9IEUA/Wang et al. - 2018 - Optimal Subsampling for Large Sample Logistic Regr.pdf}
}

@InProceedings{Ma_2014,
	title = 	 {A Statistical Perspective on Algorithmic Leveraging},
	author = 	 {Ma, Ping and Mahoney, Michael and Yu, Bin},
	booktitle = 	 {Proceedings of the 31st {I}nternational {C}onference on Machine Learning},
	pages = 	 {91--99},
	year = 	 {2014},
	editor = 	 {Xing, Eric P. and Jebara, Tony},
	volume = 	 {32},
	number =       {1},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Bejing, China},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v32/ma14.pdf},
	abstract = 	 {One popular method for dealing with large-scale data sets is sampling. Using the empirical statistical leverage scores as an importance sampling distribution, the method of algorithmic leveraging samples and rescales rows/columns of data matrices to reduce the data size before performing computations on the subproblem. Existing work has focused on algorithmic issues, but none of it addresses statistical aspects of this method.  Here, we provide an effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model.   In particular, for several versions of leverage-based sampling, we derive results for the bias and variance, both conditional and unconditional on the observed data. We show that from the statistical perspective of bias and variance, neither leverage-based sampling nor uniform sampling dominates the other. This result is particularly striking, given the well-known result that, from the algorithmic perspective of worst-case analysis, leverage-based sampling provides uniformly superior worst-case algorithmic results, when compared with uniform sampling. Based on these theoretical results, we propose and analyze two new leveraging algorithms: one constructs a smaller least-squares problem with “shrinked” leverage scores (SLEV), and the other solves a smaller and unweighted (or biased) least-squares problem (LEVUNW). The empirical results indicate that our theory is a good predictor of practical performance of existing and new leverage-based algorithms and that the new algorithms achieve improved performance.}
}

@book{villaniOptimalTransportOld2009,
  title = {Optimal Transport: Old and New},
  shorttitle = {Optimal Transport},
  author = {Villani, Cédric},
  year = {2009},
  number = {338},
  publisher = {{Springer}},
  location = {{Berlin Heidelberg}},
  isbn = {978-3-540-71049-3 978-3-662-50180-1},
  langid = {english},
  pagetotal = {973},
  doi = {10.1007/978-3-540-71050-9}
}

@article{cuturiMongeBregmanOccam2023,
  title = 	 {Monge, {{Bregman}} and {{Occam}}: {{Interpretable Optimal Transport}} in {{High-Dimensions}} with {{Feature-Sparse Maps}}},
  author =       {Cuturi, M. and Klein, Michal and Ablin, Pierre},
  Journal = 	 {Proceedings of the 40th International Conference on Machine Learning},
  year = 	 {2023},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v202/cuturi23a.html},
}

@article{gelman_bayesian_2020,
  title = {Bayesian {{Workflow}}},
  author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and Bürkner, Paul-Christian and Modrák, Martin},
  year = {2020},
  journal = {{arXiv}},
  doi = {10.48550/ARXIV.2011.01808},
  urldate = {2023-09-18},
  abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
  version = {1},
  keywords = {FOS: Computer and information sciences,Methodology (stat.ME)}
}
